# -*- coding: utf-8 -*-
"""BBC News dataset_Text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ycrpuShB2GZjCVGVBcB3f_8yVKIY-jc9
"""

#Importing all required libraries

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#using tensorflow framework for text classification

import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, Bidirectional, LSTM
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import GlobalAveragePooling1D, Conv1D

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import nltk
nltk.download("stopwords")

!pip install contractions
!pip install unidecode

#importing necessary libraries for preprocessing for NLP

from nltk.tokenize import word_tokenize
from string import punctuation
from nltk.corpus import stopwords
from contractions import fix
from unidecode import unidecode
from nltk.stem import WordNetLemmatizer
from tqdm import tqdm
import pandas 
tqdm.pandas()

#loading given csv file in dataframe df

df= pd.read_csv("bbc-text.csv")

df.head()

#checking shape of dataframe i.e 2225 rows and 2 columns

df.shape

df.groupby("category").first()

# Checking the categories of target column "category"

plt.figure(figsize=(5,1.5))
from tensorflow.python.framework.importer import op_def_registry
sns.countplot(y=df["category"], data= df, order= df["category"].value_counts().index)

# Making new feature named "label"

df["label"]= df.category

#applying label encoder for geting corresponding numbers for each category

encoder= LabelEncoder()
encoder.fit(df.label)
df["label"]= encoder.transform(df.label)

# writing a function for preprocessing on text data

def clean_data(x):
  remove_blank= x.replace("\\n", " ").replace("\t", " ")
  accented= unidecode(remove_blank)
  expand= fix(accented)
  stopwords_list= stopwords.words("english")
  tokens= word_tokenize(expand)
  clean_words= [i.lower() for i in tokens if i.lower() not in stopwords_list and(i.lower() not in punctuation) and(len(i)>2) and (i.isalpha())]
  lemmatizer= WordNetLemmatizer()
  final_text=[]
  for i in clean_words:
    lemmatized_words= lemmatizer.lemmatize(i)
    final_text.append(lemmatized_words)
  return " ".join(final_text)

>>> import nltk
  >>> nltk.download('punkt')

>>> import nltk
  >>> nltk.download('wordnet')

# Preprocessing on given data and adding new feature as "clean-text" in same dataframe

df["clean_text"]= df.text.apply(lambda x: clean_data(x))

df.head()

#Spliting the data into training and testing data

x_train,x_test,y_train,y_test= train_test_split(df.clean_text, df.label, test_size=0.2)

#word indexing
max_words=1000

tk= Tokenizer(max_words, oov_token='##oov##')

tk.fit_on_texts(x_train)

x_train_seq= tk.texts_to_sequences(x_train)
x_test_seq= tk.texts_to_sequences(x_test)

#padding sequence
#setting max length of input words as 200 and truncating(reducing) is kept as post
max_words_persent= 200
x_train_seq=pad_sequences(x_train_seq, padding="post", maxlen=max_words_persent, truncating="post")
x_test_seq=pad_sequences(x_test_seq, padding="post", maxlen=max_words_persent, truncating="post")

#Early Stopping is used to avoid overfitting when training a learner with an iterative method, such as gradient descent.
# Here Early stoppage is used when val_loss does not change for 4 iterations 
es= EarlyStopping(monitor= "val_loss", patience=4)

#Create a Sequential Model which includes Embedding, Con1D and Global Average Pooling1D layers.
# Using relu as activation in hidden layers
# as this is multiclassification problem, I have used softmax as activation function in output layer.

model= Sequential()
model.add(Embedding(input_dim=max_words+1, output_dim=5, input_length = max_words_persent))
model.add(Conv1D(128, 5, activation="relu"))
model.add(GlobalAveragePooling1D())
model.add(Dense(64, activation="relu"))
model.add(Dense(5, activation= "softmax"))

#Created model layers look like this.
model.summary()

#compiled model using 
#optimizer as adam
#loss as sparse_categorical_crossentrophy as there are multiple classes.
#Evaluation metrics as accuracy

model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# This is last step of model building in which we fit training data an also pass the test data for validation(loss on test data will be evaluated simultaniously)
# As batch size passed is 50, this is example of mini batch stochastic gradient  descent
#Early stoppage is passed in callbacks to avoid over fitting

history= model.fit(
    x_train_seq,
    y_train,
    batch_size=50,
    epochs=100,
    validation_data=(x_test_seq, y_test),
    callbacks=[es]
)

#As seen above accuracy score on training data is 100% and on test data is 92.58%
# results are good and can be considered for deployment
# Evaluation is done on test data as follows

model.evaluate(x_test_seq, y_test)

# Prediction on testing data is done as follows
# This gives the output of probability for each class
# Higher the probability, higher is the chance of text tobe classified in that class

model.predict(x_test_seq)

# Plotting graph between training loss and testing loss through all iterations

plt.plot(history.history["loss"], label="loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.legend()

# Plotting graph between training accuracy and testing accuracy through all iterations

plt.plot(history.history["accuracy"], label="accuracy")
plt.plot(history.history["val_accuracy"], label="val_accuracy")
plt.legend()

df.head(5)

df.loc[3,"text"]

text=df.loc[3,"text"]
text = clean_data(text)
padded_text=pad_sequences(tk.texts_to_sequences([text]),maxlen=200, truncating="post")

prediction=model.predict(padded_text)
for item in prediction:
  index=0
  for ele in item:
    print(encoder.inverse_transform([index])+ "â€”" +str(round(ele*100,4))+ "%")
    index+=1

# As in the above prediction for sport is 99.99% it is classified in sports which is correct as seen in dataframe


# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b1_xuS4fZwnSRq-nkAApZRgZCgcVPycR
"""

#Importing all required libraries

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

#using tensorflow framework for text classification

import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten,Dense, MaxPool2D
from tensorflow.keras.callbacks import EarlyStopping

import matplotlib.pyplot as plt

# Loading mnist data
# Spliting the data into training and testing data

(x_train, y_train),(x_test, y_test)=keras.datasets.mnist.load_data()

x_train.shape

x_test.shape

#using matplotlib.pyplot for visualization of x_train[0]
# and checking the corresponding target value in y_train

plt.imshow(x_train[0])
y_train[0]

# Each image is 2D array of size (1,28,28) ie 28 rows and 28 fetures(columns)

x_train[0]

# As image pixel ranges from 0 to 255 we need to scale it between 0 to 1

x_train_sc = x_train/255
x_test_sc= x_test/255

x_train_sc[0]

#Create a Sequential Model.
# impage is first flattened to make the multidimensional input one-dimensional
# Using relu as activation in hidden layers
# as this is multiclassification problem, I have used softmax as activation function in output layer.

model= Sequential()
model.add(Flatten(input_shape=(28,28)))
model.add(Dense(128, activation="relu"))
model.add(Dense(10, activation="softmax"))

#Created model layers look like this.

model.summary()

#compiled model using 
#optimizer as adam
#loss as sparse_categorical_crossentropy as there are multiple classes.
#Evaluation metrics as accuracy

model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics= ["accuracy"]
)

# This is last step of model building in which we fit training data an also pass the test data for validation(loss on test data will be evaluated simultaniously)
# As batch size passed is 50, this is example of mini batch stochastic gradient  descent
#Early stoppage is passed in callbacks to avoid over fitting

es= EarlyStopping(monitor="val_loss", patience=4)

history= model.fit(
    x_train_sc, 
    y_train,
    verbose= 1,
    batch_size=100,
    epochs=5,
    validation_data=(x_test_sc,y_test),
    callbacks= [es]

                   )

#As seen above accuracy score on training data is 98.05% and on test data is 97.09%
# results are good and can be considered for deployment
# Evaluation is done on test data as follows

model.evaluate(x_test_sc, y_test)

# Prediction on testing data is done as follows

model.predict(x_test_sc)

# Plotting graph between training loss and testing loss through all iterations

plt.plot(history.history["loss"], label="loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.legend()

# Plotting graph between training accuracy and testing accuracy through all iterations

plt.plot(history.history["accuracy"], label="accuracy")
plt.plot(history.history["val_accuracy"], label="val_accuracy")
plt.legend()

